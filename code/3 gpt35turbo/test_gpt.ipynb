{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using GPT-3.5-turbo model via OpenAI API to classify and generate explanations for NLI samples from SI-NLI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "np.random.seed(424)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load SI-NLI dataset\n",
    "train = pd.read_csv('data/SI-NLI/train.tsv', sep=\"\\t\")\n",
    "dev = pd.read_csv('data/SI-NLI/dev.tsv', sep=\"\\t\")\n",
    "test = pd.read_csv('data/SI-NLI/test.tsv', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select 100 samples from train set used to test the model\n",
    "t = train.sample(n=100)\n",
    "t['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load english translations of SI-NLI dataset\n",
    "train_eng = pd.read_csv('data/SI-NLI-en/train.tsv', sep='\\t')\n",
    "t_eng = train_eng[train_eng['pair_id'].isin(t['pair_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep needed columns\n",
    "t = t[['label', 'premise', 'hypothesis']]\n",
    "t_eng = t_eng[['label', 'premise', 'hypothesis']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt1si(premise, hypothesis):\n",
    "\treturn f'Glede na stavek \"{premise}\" ugotovite, ali je naslednja izjava posledica, kontradikcija ali nevtralna: \"{hypothesis}\"\\nOdgovor (posledica ali kontradikcija ali nevtralna) je:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt1en(premise, hypothesis):\n",
    "\treturn f'Given the sentence \"{premise}\", determine if the following statement is entailed or contradicted or neutral: \"{hypothesis}\"\\nThe answer (entailed or contradicted or neutral) is:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt2en(premise, hypothesis):\n",
    "\treturn f'''Instructions: You will be presented with a premise and a hypothesis about that premise in slovene. You need to decide whether the hypothesis is entailed by the premise by choosing one of the following answers: 'entailment': The hypothesis follows logically from the information contained in the premise. 'contradiction': The hypothesis is logically false from the information contained in the premise. 'neutral': It is not possible to determine whether the hypothesis is true or false without further information. Read the passage of information thoroughly and select the correct answer from the three answer labels. Read the premise thoroughly to ensure you know what the premise entails.\n",
    "\n",
    "Premise: {premise}\n",
    "Hypothesis: {hypothesis}\n",
    "\n",
    "Answer (just one word, either entailment/neutral/contradiction):'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt2si(premise, hypothesis):\n",
    "    return f'''Navodila: Predstavljena vam bo premisa in hipoteza o tej premisi. Odločiti se morate, ali je hipoteza posledica premise, tako da izberete enega od naslednjih odgovorov: 'posledica': Hipoteza logično sledi iz informacij, ki jih vsebuje premisa. 'kontradikcija': Hipoteza je logično napačna glede na informacije, ki jih vsebuje premisa. 'nevtralno': Brez dodatnih informacij ni mogoče ugotoviti, ali je hipoteza resnična ali napačna. Natančno preberite odlomek informacij in izberite pravilen odgovor med tremi oznakami odgovorov. Temeljito preberite predpostavko, da boste vedeli, kaj sledi iz predpostavke.\n",
    "\n",
    "Premisa: {premise}\n",
    "Hipoteza: {hypothesis}\n",
    "\n",
    "Odgovor (samo ena beseda, bodisi posledica/nevtralno/kontradikcija):'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "with open(\"API_KEY_OPENAI\", encoding='utf-8') as f:\n",
    "  openai.api_key = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_request(prompt):\n",
    "\tresponse = openai.ChatCompletion.create(\n",
    "\t\trequest_timeout=15,\n",
    "\t\tmodel=\"gpt-3.5-turbo\",\n",
    "\t\tmessages=[{'role': 'user', 'content': prompt}],\n",
    "\t\ttemperature=0,\n",
    "\t\tmax_tokens=20,\n",
    "\t\tstop=[\"\\n\"]\n",
    "\t)\n",
    "\n",
    "\treturn response.choices[0].message.content, response.usage.total_tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_multiple_requests(df, prompt):\n",
    "\tresponses = []\n",
    "\ttokens = []\n",
    "\tfor i, row in tqdm(df.iterrows()):\n",
    "\t\tresponse, token = gpt_request(prompt(row['premise'], row['hypothesis']))\n",
    "\t\tresponses.append(response)\n",
    "\t\ttokens.append(token)\n",
    "\treturn responses, sum(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract classification from responses\n",
    "def process_slo(answer):\n",
    "\tanswer = answer.lower()\n",
    "\tif 'posledica' in answer:\n",
    "\t\treturn 'entailment'\n",
    "\telif 'kontradikcija' in answer:\n",
    "\t\treturn 'contradiction'\n",
    "\telif 'nevtral' in answer:\n",
    "\t\treturn 'neutral'\n",
    "\telse:\n",
    "\t\treturn answer\n",
    "\n",
    "def batch_process_slo(answers):\n",
    "\treturn [process_slo(answer) for answer in answers]\n",
    "\n",
    "def process_eng(answer):\n",
    "\tanswer = answer.lower()\n",
    "\tif 'entail' in answer:\n",
    "\t\treturn 'entailment'\n",
    "\telif 'contradict':\n",
    "\t\treturn 'contradiction'\n",
    "\telif 'neutral' in answer:\n",
    "\t\treturn 'neutral'\n",
    "\telse:\n",
    "\t\treturn answer\n",
    "\t\n",
    "def batch_process_eng(answers):\n",
    "\treturn [process_eng(answer) for answer in answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def confusion_matrix_for_column(df, col):\n",
    "\tcm = confusion_matrix(df['label'], df[col], labels=['entailment', 'neutral', 'contradiction'])\n",
    "\tsns.heatmap(cm, annot=True, xticklabels=['entailment', 'neutral', 'contradiction'], yticklabels=['entailment', 'neutral', 'contradiction'])\n",
    "\tplt.xlabel('Predicted')\n",
    "\tplt.ylabel('True')\n",
    "\tplt.title('Confusion matrix for ' + col)\n",
    "\tplt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []\n",
    "tokens = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt1si # one of the four prompt functions to use\n",
    "df = t # dataset to use\n",
    "\n",
    "for i, row in tqdm(df.iterrows()):\n",
    "\tresponse, token = gpt_request(prompt(row['premise'], row['hypothesis']))\n",
    "\tresponses.append(response)\n",
    "\ttokens += token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_processed = batch_process_slo(responses) # extract classification from responses\n",
    "t['prompt1si'] = responses_processed # add to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many correct\n",
    "print(sum(t['label'] == t['prompt1si']))\n",
    "print(t['prompt1si'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_for_column(t, 'prompt1si')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few Shot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selects k samples from each label randomly\n",
    "def get_k_each(df, k, random_state=42):\n",
    "    samples_e = df[df['label'] == 'entailment'].sample(k, random_state=random_state)\n",
    "    samples_n = df[df['label'] == 'neutral'].sample(k, random_state=random_state)\n",
    "    samples_c = df[df['label'] == 'contradiction'].sample(k, random_state=random_state)\n",
    "    return pd.concat([samples_e, samples_n, samples_c]).sample(frac=1, random_state=random_state).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_label_to_eng = {'entailment': 'entailed', 'neutral': 'neutral', 'contradiction': 'contradicted'}\n",
    "\n",
    "# generates prompt for few shot prompting from examples and premise, hypothesis\n",
    "def few_shot_prompt(samples, premise, hypothesis):\n",
    "\tprompt = ''\n",
    "\tfor\ti, row in samples.iterrows():\n",
    "\t\tprompt += prompt1en(row['premise'], row['hypothesis'])\n",
    "\t\tprompt += map_label_to_eng[row['label']] + '\\n\\n'\n",
    "\tprompt += prompt1en(premise, hypothesis)\n",
    "\treturn prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 selections for 3 shot prompting (1 example per class)\n",
    "samples3_1 = get_k_each(train, 1, 42)\n",
    "samples3_2 = get_k_each(train, 1, 52)\n",
    "samples3_3 = get_k_each(train, 1, 62)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []\n",
    "tokens = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = samples3_1 # samples to use for few-shot learning\n",
    "df = t # dataframe to use for testing\n",
    "for i in tqdm(range(len(responses), len(df))):\n",
    "\trow = df.iloc[i]\n",
    "\tresponse, token = gpt_request(few_shot_prompt(samples, row['premise'], row['hypothesis']))\n",
    "\tresponses.append(response)\n",
    "\ttokens += token\n",
    "\tsleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_processed = batch_process_eng(responses)\n",
    "t['few3_1'] = responses_processed\n",
    "\n",
    "# count how many correct\n",
    "print(sum(t['label'] == t['few3_1']))\n",
    "print(t['few3_1'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_for_column(t, 'few3_1')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_expl(premise, hypothesis):\n",
    "\treturn f'''Given the sentence \"{premise}\", determine if the following statement is entailed or contradicted or neutral: \"{hypothesis}\". First give a short, one sentence reasoning or explanation for the decision in slovene, and then the final answer (one english word - \"entailed\" or \"contradicted\" or \"neutral\") in a new line after that.\n",
    "\n",
    "Razlaga v slovenščini:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_request_expl(prompt):\n",
    "\tresponse = openai.ChatCompletion.create(\n",
    "\t\trequest_timeout=15,\n",
    "\t\tmodel=\"gpt-3.5-turbo\",\n",
    "\t\tmessages=[{'role': 'user', 'content': prompt}],\n",
    "\t\ttemperature=0,\n",
    "\t\tmax_tokens=512,\n",
    "\t)\n",
    "\n",
    "\treturn response.choices[0].message.content, response.usage.total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracts the classification from the response of the model\n",
    "def process_eng_expl(response):\n",
    "\tanswer = response.strip().split('\\n')[-1]\n",
    "\tanswer = answer.lower()\n",
    "\tif 'entail' in answer or 'potrjen' in answer:\n",
    "\t\treturn 'entailment'\n",
    "\telif 'contradict' in answer or 'kontra' in answer:\n",
    "\t\treturn 'contradiction'\n",
    "\telif 'neutral' in answer or 'nevt' in answer:\n",
    "\t\treturn 'neutral'\n",
    "\telse:\n",
    "\t\treturn answer\n",
    "\t\n",
    "\n",
    "# extracts the explanation from the response of the model\n",
    "def extract_explanation(response):\n",
    "\tresponse = '\\n'.join(response.strip().split('\\n')[:-1]).strip()\n",
    "\treturn response\n",
    "\t\n",
    "def batch_process_eng_expl_preds(answers):\n",
    "\treturn [process_eng_expl(answer) for answer in answers]\n",
    "\n",
    "def batch_process_eng_expl(answers):\n",
    "\treturn [extract_explanation(answer) for answer in answers]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []\n",
    "tokens = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"temp\", \"rb\") as fp:\n",
    "\tb = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"temp\", \"wb\") as fp:\n",
    "\tpickle.dump(responses, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = t\n",
    "for i in tqdm(range(len(responses), len(df))):\n",
    "\trow = df.iloc[i]\n",
    "\tresponse, token = gpt_request_expl(prompt_expl(row['premise'], row['hypothesis']))\n",
    "\tresponses.append(response)\n",
    "\ttokens += token\n",
    "\t\n",
    "\twith open(\"temp\", \"wb\") as fp:\n",
    "\t\tpickle.dump(responses, fp)\n",
    "\tsleep(0.01)\n",
    "\n",
    "responses_expl = responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_expl_preds_processed = batch_process_eng_expl_preds(responses_expl)\n",
    "responses_expl_processed = batch_process_eng_expl(responses_expl)\n",
    "t['expl_preds'] = responses_expl_preds_processed\n",
    "t['expl'] = responses_expl_processed\n",
    "\n",
    "# count how many correct\n",
    "print(sum(t['label'] == t['expl_preds']))\n",
    "print(t['expl_preds'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_for_column(t, 'expl_preds')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selects 50 samples from the test set and saves them to a file. To use for grading explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_e = t[t['label'] == 'entailment'].sample(18, random_state=442)\n",
    "sample_c = t[t['label'] == 'contradiction'].sample(16, random_state=442)\n",
    "sample_n = t[t['label'] == 'neutral'].sample(16, random_state=442)\n",
    "sample = pd.concat([sample_e, sample_c, sample_n]).sample(frac=1, random_state=442)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.to_csv('data/explanations_sample.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English zero shot\n",
    "\n",
    "Using english translations of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []\n",
    "tokens = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt1en\n",
    "df = t\n",
    "for i, row in tqdm(df.iterrows()):\n",
    "\tresponse, token = gpt_request(prompt(row['premise'], row['hypothesis']))\n",
    "\tresponses.append(response)\n",
    "\ttokens += token\n",
    "\n",
    "responses_1en = responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_1en_processed = batch_process_eng(responses_1en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_eng['prompt1en_eng'] = responses_1en_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many correct\n",
    "print(sum(t_eng['label'] == t_eng['prompt1en_eng']))\n",
    "print(t_eng['prompt1en_eng'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_for_column(t_eng, 'prompt1en_eng')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
